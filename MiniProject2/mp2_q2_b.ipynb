{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3n3Awkwj7Z8W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_Nlx_23P7Z5K"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oJJXunf7Z26",
        "outputId": "5893cfa8-ae82-447a-e2f8-010cd78852c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BPn0fJ1by9Ve"
      },
      "outputs": [],
      "source": [
        "def count_true(a1, a2):\n",
        "  e = torch.eq(a1, a2)\n",
        "  e = e.cpu().numpy()\n",
        "  c = np.count_nonzero(e)\n",
        "  return c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Pn05zBuxxxMe"
      },
      "outputs": [],
      "source": [
        "def preprocess():\n",
        "   # load the text file\n",
        "    data = open(\"dataset.txt\", 'r').read()\n",
        "    chars = sorted(list(set(data)))\n",
        "    data_size, vocab_size = len(data), len(chars)\n",
        "    \n",
        "    # char to index and index to char maps\n",
        "   \n",
        "    char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "    ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "    \n",
        "    # convert data from chars to indices\n",
        "    data = list(data)\n",
        "    for i, ch in enumerate(data):\n",
        "        data[i] = char_to_ix[ch]\n",
        "    # data tensor on device\n",
        "    data = torch.tensor(data).to(device)\n",
        "    data = torch.unsqueeze(data, dim=1)\n",
        "    \n",
        "    return data , ix_to_char , data_size, vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "N250OR8q7Z0Y"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, input_size)\n",
        "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input_seq, hidden_state):\n",
        "        embedding = self.embedding(input_seq)\n",
        "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
        "        output = self.fc(output)\n",
        "        return output, (hidden_state[0].detach(), hidden_state[1].detach())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aBDr7jEMe1oj"
      },
      "outputs": [],
      "source": [
        "def test(data,data_size,rnn,ix_to_char):\n",
        "\n",
        "        data_ptr = 0\n",
        "        hidden_state = None\n",
        "        # random character\n",
        "        rand_index = np.random.randint(data_size-1)\n",
        "        input_seq = data[rand_index : rand_index+1]\n",
        "        \n",
        "        for i in range(400):\n",
        "          \n",
        "            # forward pass\n",
        "            output, hidden_state = rnn(input_seq, hidden_state)\n",
        "            \n",
        "            # construct categorical distribution and sample a character\n",
        "            output = F.softmax(torch.squeeze(output), dim=0)\n",
        "            dist = Categorical(output)\n",
        "            index = dist.sample()\n",
        "            \n",
        "            # print the sampled character\n",
        "            print(ix_to_char[index.item()], end='')\n",
        "            \n",
        "            # next input is current output\n",
        "            input_seq[0][0] = index.item()\n",
        "            data_ptr += 1\n",
        "          "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data , ix_to_char, data_size, vocab_size,rnn,epochs,seq_len,loss_fn,optimizer):\n",
        "    acc = []\n",
        "    loss_list = []\n",
        "    for i_epoch in range(1, epochs+1):\n",
        "        \n",
        "        # random starting point (1st 100 chars) from data to begin\n",
        "        data_ptr = np.random.randint(100)\n",
        "        n = 0\n",
        "        running_loss = 0\n",
        "        hidden_state = None\n",
        "        true_predicts = 0\n",
        "        while data_ptr + seq_len + 1 < data_size:\n",
        "          \n",
        "            input_seq = data[data_ptr : data_ptr+seq_len]\n",
        "            target_seq = data[data_ptr+1 : data_ptr+seq_len+1]\n",
        "            target_seq_onehot = torch.squeeze(target_seq)\n",
        "            \n",
        "            # forward pass\n",
        "            output, hidden_state = rnn(input_seq, hidden_state)\n",
        "            # compute loss\n",
        "            loss = loss_fn(F.softmax(torch.squeeze(output), dim=1), F.softmax(torch.squeeze(output), dim=1))\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            output = F.softmax(torch.squeeze(output), dim=1)\n",
        "            dist = Categorical(output)\n",
        "            index = dist.sample()\n",
        "            \n",
        "            \n",
        "            # compute gradients and take optimizer step\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # compute true predicts\n",
        "            true_predicts += count_true(torch.squeeze(target_seq), index)\n",
        "            # update the data pointer\n",
        "            data_ptr += seq_len\n",
        "            n +=1\n",
        "           \n",
        "        acc.append(true_predicts*100/n)  \n",
        "        loss_list.append(running_loss/n)  \n",
        "        # print loss and save weights after every epoch\n",
        "        print(\"Epoch: {0} \\t Loss: {1:.4f} \\t accuracy: {2:.4f}\".format(i_epoch, running_loss/n,true_predicts*100/data_size))\n",
        "        \n",
        "    return acc,loss_list\n"
      ],
      "metadata": {
        "id": "74VQM9WfaMtV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KLDivLoss"
      ],
      "metadata": {
        "id": "hXVdC1zb1wWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def runn(hidden_size, seq_len, num_layers, lr, epochs):\n",
        "  # Hyperparameters\n",
        "  hidden_size = hidden_size  \n",
        "  # length of LSTM sequence \n",
        "  seq_len = seq_len\n",
        "  # num of layers in LSTM layer stack      \n",
        "  num_layers =num_layers      \n",
        "  lr = lr   \n",
        "  epochs = epochs\n",
        "  data , ix_to_char, data_size, vocab_size =  preprocess()\n",
        "  # model \n",
        "  rnn = RNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n",
        "  # loss function and optimizer\n",
        "  loss_fn = nn.KLDivLoss()\n",
        "  optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
        "\n",
        "  acc_list,loss_list = train(data , ix_to_char, data_size, vocab_size,rnn,epochs,seq_len,loss_fn,optimizer)\n",
        "  print(\"generate text ------------------------------\")\n",
        "  test(data,data_size,rnn,ix_to_char)\n",
        "  \n",
        "  return acc_list,loss_list"
      ],
      "metadata": {
        "id": "GPbUUQpfWOgW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_list,loss_list = runn(512, 250, 3, 0.001, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPnf_ZrYUkvr",
        "outputId": "4eec159a-3bda-4990-d63a-e15b3cc8f79d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2887: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \t Loss: -0.0549 \t accuracy: 1.2434\n",
            "Epoch: 2 \t Loss: -0.0549 \t accuracy: 1.2495\n",
            "Epoch: 3 \t Loss: -0.0549 \t accuracy: 1.2457\n",
            "Epoch: 4 \t Loss: -0.0549 \t accuracy: 1.2623\n",
            "Epoch: 5 \t Loss: -0.0549 \t accuracy: 1.2197\n",
            "Epoch: 6 \t Loss: -0.0549 \t accuracy: 1.2459\n",
            "Epoch: 7 \t Loss: -0.0549 \t accuracy: 1.2457\n",
            "Epoch: 8 \t Loss: -0.0549 \t accuracy: 1.2516\n",
            "Epoch: 9 \t Loss: -0.0549 \t accuracy: 1.2443\n",
            "Epoch: 10 \t Loss: -0.0549 \t accuracy: 1.2439\n",
            "generate text ------------------------------\n",
            "BKpS.M0c moVVjpvW_- )GS9aF3},J.;rAS0gdrWcKOLixS4k\n",
            "-F1ghcCPSav0fT1vre)FjIdIT-gpJ4:L.y,A9m4\"nYL.qWp.hx7TE}!G9NyK4D\"O}^J\"!}ZfN\n",
            "nSqs•G•6!Sf'sMcZ!FbA' hDU7)f3}laZA_G\tK(QD!r\"r_^OBF3DIy/Q0E3,qf6XA(\n",
            "}:•/s.?,ZFc!^) DRz}7cOLZa;0ZidJ_(Oi;d\tES!(C1/kTzat1fYjeuI36C)2K,U.ZaVqNDvi-. ;'r\th' sU.q3FcEYsR^Dde\t9tOkZBLit)•^•kCXBfQ2nTrw•kj9cwEnCT))QUF?ny/dRfe0V_aCLrWye4gn-7.9eEwo0pHg\tPE}UY)/L2JDue^SO7grqdE.i4\"xzNIcpKT_d"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLLLoss"
      ],
      "metadata": {
        "id": "8BCcAXYT1boZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data , ix_to_char, data_size, vocab_size,rnn,epochs,seq_len,loss_fn,optimizer):\n",
        "    acc = []\n",
        "    loss_list = []\n",
        "    for i_epoch in range(1, epochs+1):\n",
        "        \n",
        "        # random starting point (1st 100 chars) from data to begin\n",
        "        data_ptr = np.random.randint(100)\n",
        "        n = 0\n",
        "        running_loss = 0\n",
        "        hidden_state = None\n",
        "        true_predicts = 0\n",
        "\n",
        "        while data_ptr + seq_len + 1 < data_size:\n",
        "          \n",
        "            input_seq = data[data_ptr : data_ptr+seq_len]\n",
        "            target_seq = data[data_ptr+1 : data_ptr+seq_len+1]\n",
        "            target_seq = torch.squeeze(target_seq)\n",
        "            \n",
        "            # forward pass\n",
        "            output, hidden_state = rnn(input_seq, hidden_state)\n",
        "            loss = loss_fn(torch.squeeze(output), torch.squeeze(target_seq))\n",
        "            running_loss += loss.item()\n",
        "            # compute loss\n",
        "            output = F.softmax(torch.squeeze(output), dim=1)\n",
        "            dist = Categorical(output)\n",
        "            index = dist.sample()\n",
        "            \n",
        "            \n",
        "            # compute gradients and take optimizer step\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # compute true predicts\n",
        "            true_predicts += count_true(torch.squeeze(target_seq), index)\n",
        "            # update the data pointer\n",
        "            data_ptr += seq_len\n",
        "            n +=1\n",
        "           \n",
        "        acc.append(true_predicts*100/n)  \n",
        "        loss_list.append(running_loss/n)  \n",
        "        # print loss and save weights after every epoch\n",
        "        print(\"Epoch: {0} \\t Loss: {1:.4f} \\t accuracy: {2:.4f}\".format(i_epoch, running_loss/n,true_predicts*100/data_size))\n",
        "        \n",
        "    return acc,loss_list\n"
      ],
      "metadata": {
        "id": "XRbKOQRf2CCl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def runn(hidden_size, seq_len, num_layers, lr, epochs):\n",
        "  # Hyperparameters\n",
        "  hidden_size = hidden_size  \n",
        "  # length of LSTM sequence \n",
        "  seq_len = seq_len\n",
        "  # num of layers in LSTM layer stack      \n",
        "  num_layers =num_layers      \n",
        "  lr = lr   \n",
        "  epochs = epochs\n",
        "  data , ix_to_char, data_size, vocab_size =  preprocess()\n",
        "  # model \n",
        "  rnn = RNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n",
        "  # loss function and optimizer\n",
        "  loss_fn = nn.NLLLoss()\n",
        "  optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
        "\n",
        "  acc_list,loss_list = train(data , ix_to_char, data_size, vocab_size,rnn,epochs,seq_len,loss_fn,optimizer)\n",
        "  print(\"generate text ------------------------------\")\n",
        "  test(data,data_size,rnn,ix_to_char)\n",
        "  \n",
        "  return acc_list,loss_list"
      ],
      "metadata": {
        "id": "oyIHlTr-1azn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_list,loss_list = runn(512, 150, 3, 0.01, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu1utxSR1hbv",
        "outputId": "9ff2e200-8763-4add-9d82-1ba3f74cc285"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \t Loss: -9183.8288 \t accuracy: 17.6342\n",
            "Epoch: 2 \t Loss: -27571.6156 \t accuracy: 17.7413\n",
            "Epoch: 3 \t Loss: -46060.6591 \t accuracy: 17.7416\n",
            "Epoch: 4 \t Loss: -64626.3289 \t accuracy: 17.7414\n",
            "Epoch: 5 \t Loss: -83299.3643 \t accuracy: 17.7416\n",
            "generate text ------------------------------\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "mp2_q2_b_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}